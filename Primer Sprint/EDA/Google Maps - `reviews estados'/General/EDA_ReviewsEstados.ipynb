{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# **<center>PROYECTO FINAL: GOOGLE & YELP<center>**\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2ApfzV-HK8jS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mediante el siguiente Notebook se realiza un análisis exploratorio(EDA) de los datasets correspondientes a las reviews obtenidas de la plataforma google maps, se trabajarán los datos de unicamente 4 estados , Pensilvania, Florida, California e Ilinois, los mismos fueron seleccionados en base a la cantidad de negocios por estados."
      ],
      "metadata": {
        "id": "TPC3gNtmzYD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar librerias a utilizar\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n"
      ],
      "metadata": {
        "id": "qkb4Yi9Eahxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-3yCccz5Csw",
        "outputId": "9d43d621-9533-45a8-f9f3-a98061eaeb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer conexión con google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGTzsoKga3Ms",
        "outputId": "7128f1c9-54fe-4590-f1c8-640e567d858a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Lectura de datos - \"Google maps-reviews-estados\"*\n",
        "---\n"
      ],
      "metadata": {
        "id": "OVheo72WL2Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura de datos\n",
        "ruta_reviews_estados='/content/drive/MyDrive/Proyecto Final/Google Maps/reviews-estados/review-California/1.json' # reseñas del estado de California\n",
        "reviews_california=pd.read_json(ruta_reviews_estados,lines=True)\n",
        "reviews_california"
      ],
      "metadata": {
        "id": "fAe3Tkz1bm3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lee los datos de \"user_reviews\" extraidos de Google Maps y retorna un dataframe de pandas con los datos\n",
        "def download_and_read(file_id):\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'google.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "    return pd.read_json(output, lines=True)\n",
        "\n",
        "# Reemplaza 'file_id' con el ID de tu archivo en Google Drive\n",
        "PA = download_and_read('1JOP4_XxxRGPsqTl0tZHNXYQg221qM24p')\n",
        "FL = download_and_read('1cFw3IyigV53ngxfBy4U9VJfu8yILSHcv')\n",
        "CA = download_and_read('13JlGdagtTp4SrUIXu5osayX0f-vmeMz6')\n",
        "IL = download_and_read('1ZOJjSC6cudmNj8W3PBuKLvQEr18R7akt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGytzOvn4Ao5",
        "outputId": "e2514a32-e16e-469a-813f-4a005ed133f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JOP4_XxxRGPsqTl0tZHNXYQg221qM24p\n",
            "To: /content/google.json\n",
            "100%|██████████| 45.1M/45.1M [00:00<00:00, 57.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
            "  return values.astype(dtype, copy=copy)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cFw3IyigV53ngxfBy4U9VJfu8yILSHcv\n",
            "To: /content/google.json\n",
            "100%|██████████| 56.1M/56.1M [00:00<00:00, 110MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
            "  return values.astype(dtype, copy=copy)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=13JlGdagtTp4SrUIXu5osayX0f-vmeMz6\n",
            "To: /content/google.json\n",
            "100%|██████████| 47.9M/47.9M [00:00<00:00, 58.9MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
            "  return values.astype(dtype, copy=copy)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZOJjSC6cudmNj8W3PBuKLvQEr18R7akt\n",
            "To: /content/google.json\n",
            "100%|██████████| 47.0M/47.0M [00:00<00:00, 58.6MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py:189: RuntimeWarning: invalid value encountered in cast\n",
            "  return values.astype(dtype, copy=copy)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir los DataFrames de cada estado\n",
        "df_PAFLCAIL = pd.concat([PA, FL, CA, IL], ignore_index=True)"
      ],
      "metadata": {
        "id": "a6vatFg94BAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_PAFLCAIL.head()"
      ],
      "metadata": {
        "id": "uQzD8RCg4DB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A continuación se cruzarán los datos de \"user_reviews\" con los datos de \"Metadata_sitios\" y se realizará un analisis exploratorio de los datos resultantes."
      ],
      "metadata": {
        "id": "WPba7F_00xdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lee los datos del dataset \"metadata_sitios\" y devuelve un dataframe de pandas\n",
        "def download_and_read(file_id):\n",
        "    url = f'https://drive.google.com/uc?id={file_id}'\n",
        "    output = 'googlemetadata.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "    return pd.read_json(output, lines=True)\n",
        "\n",
        "# Reemplaza 'file_id' con el ID de tu archivo en Google Drive\n",
        "metadata = download_and_read('1OnyhmyG8xzdn4XU9LYcnwzBseB1_rChS')"
      ],
      "metadata": {
        "id": "VQxALvX04d0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_PAFLCAIL_final = df_PAFLCAIL[['text', 'gmap_id']]"
      ],
      "metadata": {
        "id": "wNfe0dFE4dQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata.columns"
      ],
      "metadata": {
        "id": "l_evwZe54duR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata.head()\n"
      ],
      "metadata": {
        "id": "G7SysLe34doQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecciona las columnas 'gmap_id' y 'category' de metadata\n",
        "metadata_selected = metadata[['gmap_id', 'category']]\n",
        "\n",
        "# Selecciona las columnas 'gmap_id' y 'text' de df_PAFLCAIL\n",
        "df_PAFLCAIL_selected = df_PAFLCAIL[['gmap_id', 'text']]\n",
        "\n",
        "# Une los dos DataFrames en base a 'gmap_id'\n",
        "df_merged = pd.merge(metadata_selected, df_PAFLCAIL_selected, on='gmap_id')"
      ],
      "metadata": {
        "id": "aGYxNYxe4s8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged.head()"
      ],
      "metadata": {
        "id": "YpfcmMGV4s3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'explode' convierte listas en filas separadas\n",
        "df_exploded = df_merged.explode('category')\n",
        "\n",
        "# Ahora puedes contar los valores únicos\n",
        "unique_values = df_exploded['category'].nunique()\n",
        "\n",
        "print(f\"Valores únicos en 'category': {unique_values}\")"
      ],
      "metadata": {
        "id": "Kli1mwRA4szB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_values_per_column = df_merged.count()\n",
        "print(total_values_per_column)"
      ],
      "metadata": {
        "id": "R9jZ7EOa4stQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palabras_clave = ['Burger', 'BBQ', 'Tex-Mex', 'Soul Food', 'Cajun', 'Creole', 'Hot Dog', 'Buffalo Wings', 'Clam Chowder', 'Gumbo', 'Jambalaya', 'Hushpuppies', 'Cheesesteak', 'Philly Pretzel', 'Lobster Roll', 'New England Clam Bake', 'Chicago Deep Dish Pizza', 'Thin Crust Pizza', 'Bagel', 'Reuben Sandwich', 'Po boy', 'Tater Tots', 'Cornbread', 'Fried Chicken', 'Chicken and Waffles', 'Biscuits and Gravy', 'Grits', 'Shrimp and Grits', 'Collard Greens', 'Pulled Pork', 'Ribs', 'Brisket', 'Sloppy Joe', 'Chili', 'Buffalo Chicken', 'Fried Green Tomatoes', 'Pecan Pie', 'Key Lime Pie', 'Pumpkin Pie', 'Apple Pie', 'Peach Cobbler', 'Banana Pudding', 'Red Velvet Cake', 'Gumbo', 'Shrimp Po Boy', 'Beignets', 'Tacos', 'Burritos', 'Enchiladas', 'Nachos', 'Quesadillas', 'Guacamole', 'Salsa', 'Margarita', 'Mojito', 'Mint Julep', 'Craft Beer', 'Microbrewery', 'Distillery', 'Bourbon', 'Whiskey', 'Moonshine', 'Sweet Tea', 'Iced Coffee', 'Pumpkin Spice Latte', 'Corn Dog', 'State Fair Food', 'Food Truck', 'Food Festival', 'Street Food', 'Farmers Market', 'Apple Cider', 'Clam Bake', 'Lobster Bake', 'Lobster Mac and Cheese', 'Lobster Bisque', 'Crab Cakes', 'Maryland Crab Soup', 'Chesapeake Bay Oysters', 'Manhattan Clam Chowder', 'Lobster Newberg', 'Manhattan', 'Martini', 'Craft Cocktail', 'Moon Pie', 'Fried Oreos', 'Pimento Cheese', 'Shrimp Cocktail', 'Oyster Rockefeller', 'Oyster PoBoy', 'Smoked Salmon Bagel', 'Bagel and Lox', 'Pretzel Bun', 'Buffalo Chicken Dip', 'Tuna Melt', 'Caesar Salad', 'Cobb Salad', 'Waldorf Salad', 'Monte Cristo Sandwich', 'King Cake', 'Grits', 'Pimento Cheese Grits', 'Chicken Fried Steak', 'Chicken Pot Pie', 'California Roll', 'Philly Roll', 'Sushi', 'Ramen', 'Dim Sum', 'General Tso s Chicken', 'Peking Duck', 'Egg Drop Soup', 'Chop Suey', 'Moo Shu Pork', 'California Burrito', 'Fish Tacos', 'Avocado Toast', 'Quinoa Bowl', 'Acai Bowl', 'Poke Bowl', 'Kale Salad', 'Green Goddess Dressing', 'Artisanal Cheese', 'Charcuterie', 'Truffle Fries', 'Lobster Risotto', 'Shrimp and Grits', 'Bourbon Bacon Jam', 'Sriracha', 'Sriracha Mayo', 'Ranch Dressing', 'Blue Cheese Dressing', 'Thousand Island Dressing', 'Barbecue Sauce', 'Hot Sauce', 'Mustard', 'Ketchup', 'Mayonnaise', 'Poutine', 'Maple Syrup', 'Clamato', 'Vodka Infusions', 'Avocado Ice Cream', 'Maple Bacon Ice Cream', 'Deep-Fried Ice Cream', 'Beignet Sundae', 'Biscuit Donuts', 'Fried Pickles', 'Buffalo Cauliflower', 'Gouda Mac and Cheese', 'Pulled Pork Nachos', 'Bratwurst', 'Bison Burger', 'Elk Burger', 'Duck Confit', 'Chicken and Dumplings', 'Buttermilk Pie', 'Red Beans and Rice', 'Collard Greens with Ham Hock', 'Bourbon Pecan Pie', 'Bourbon Balls', 'Moonshine Jelly', 'Bourbon Bacon Jam', 'Gator Tail', 'Crawfish Boil', 'Andouille Sausage', 'Fried Green Tomato BLT', 'Catfish Po Boy', 'Shrimp Gumbo', 'Alligator Bites', 'Grilled Cheese with Tomato Soup', 'Chicken Parmesan', 'Chicken Marsala', 'Chicken Alfredo', 'Caesar Wrap', 'Club Sandwich', 'Caprese Salad', 'Clam Linguine', 'Crab Rangoon', 'Crawfish Etouffee', 'Cuban Sandwich', 'Dutch Baby Pancake', 'Elk Chili', 'Fish and Chips', 'French Dip', 'Fried Catfish', 'Frog Legs', 'Gator PoBoy', 'General Tsos Tofu', 'Grits and Grillades', 'Hoppin John', 'Hot Chicken', 'King Ranch Casserole', 'Livermush', 'Matzo Ball Soup', 'Monte Cristo Sandwich', 'Muffuletta', 'Nashville Hot Chicken', 'Navajo Taco', 'Okra Fritters', 'Pecan Pralines', 'Pickled Okra', 'Pig Pickin', 'Rattlesnake Chili', 'Rocky Mountain Oysters', 'Shrimp and Okra Gumbo', 'Smothered Pork Chops', 'Sonoran Hot Dog', 'Sourdough Bread Bowl', 'Southern Fried Catfish', 'Spam Musubi', 'Squash Casserole', 'Tamales', 'Turducken', 'White BBQ Sauce', 'White Castle Slider', 'Zucchini Bread']  # Asegúrate de reemplazar '...' con tus palabras clave\n",
        "\n",
        "# Crea una expresión regular a partir de las palabras clave\n",
        "patron = '|'.join(palabras_clave)\n",
        "\n",
        "# Crea una nueva columna que indica si la categoría contiene alguna de las palabras clave\n",
        "df_merged['es_restaurante'] = df_merged['category'].apply(lambda x: bool(re.search(patron, str(x), re.IGNORECASE)) if x is not None else False)\n",
        "\n",
        "# Filtra solo los restaurantes\n",
        "df_restaurantes_merged = df_merged[df_merged['es_restaurante']]\n",
        "\n",
        "# Repite el proceso para df_exploded si es necesario\n",
        "df_exploded['es_restaurante'] = df_exploded['category'].apply(lambda x: bool(re.search(patron, str(x), re.IGNORECASE)) if x is not None else False)\n",
        "df_restaurantes_exploded = df_exploded[df_exploded['es_restaurante']]"
      ],
      "metadata": {
        "id": "gbpCEJ7Q41EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'Explota' la columna 'category' para que cada elemento de las listas tenga su propia fila\n",
        "df_exploded = df_restaurantes_merged.explode('category')\n",
        "\n",
        "# Ahora puedes contar los valores únicos\n",
        "num_categorias_unicas = df_exploded['category'].nunique()\n",
        "print(f\"La cantidad de valores únicos en la columna 'category' es: {num_categorias_unicas}\")"
      ],
      "metadata": {
        "id": "YtAyScZt41Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora puedes contar el total de valores\n",
        "num_categorias = df_exploded['category'].count()\n",
        "print(f\"La cantidad total de valores en la columna 'category' es: {num_categorias}\")"
      ],
      "metadata": {
        "id": "Rjuc5IGy40_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora puedes contar el total de valores\n",
        "num_categorias = df_exploded['es_restaurante'].count()\n",
        "print(f\"La cantidad total de valores en la columna 'category' es: {num_categorias}\")"
      ],
      "metadata": {
        "id": "KAGUgBnJ409h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtiene una lista de los valores en la columna 'category'\n",
        "lista_categorias = df_exploded['category'].tolist()\n",
        "\n",
        "# Imprime la lista de categorías\n",
        "print(lista_categorias)"
      ],
      "metadata": {
        "id": "34x5C5ZV407a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtiene una serie con los recuentos de valores únicos\n",
        "recuentos = df_exploded['category'].value_counts()\n",
        "\n",
        "# Obtiene las 10 palabras más repetidas\n",
        "top_10 = recuentos.head(10)\n",
        "\n",
        "# Imprime el top 10 de palabras más repetidas\n",
        "print(top_10)"
      ],
      "metadata": {
        "id": "KcuDj_Cj400I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "Wf22fcLs4ski"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crea un analizador de sentimiento\n",
        "sia = SentimentIntensityAnalyzer()\n"
      ],
      "metadata": {
        "id": "AIComW9G5dAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica el analizador de sentimiento a cada texto\n",
        "df_restaurantes_merged['sentiment_score'] = df_restaurantes_merged['text'].apply(lambda text: sia.polarity_scores(text)['compound'] if text is not None else 0)\n",
        "\n",
        "# Ajusta las puntuaciones de sentimiento para que se ajusten a la escala de -2 a 2\n",
        "df_restaurantes_merged['sentiment_score_scaled'] = df_restaurantes_merged['sentiment_score'].apply(lambda score: np.interp(score, [-1, 1], [-2, 2]))\n",
        "\n",
        "# Asigna cada rango a una etiqueta de sentimiento\n",
        "bins = [-np.inf, -1.2, -0.2, 0.2, 1.2, np.inf]\n",
        "labels = ['pesimo', 'malo', 'neutral', 'bueno', 'excelente']\n",
        "df_restaurantes_merged['sentiment_label'] = pd.cut(df_restaurantes_merged['sentiment_score_scaled'], bins=bins, labels=labels)\n",
        "\n",
        "# Ahora, la columna 'sentiment_label' contiene la etiqueta de sentimiento para cada texto\n"
      ],
      "metadata": {
        "id": "aOSuxIB35c2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Haz una copia profunda del DataFrame\n",
        "df = df_restaurantes_merged.copy(deep=True)\n",
        "\n",
        "# Aplica el analizador de sentimiento a cada texto\n",
        "df['sentiment_score'] = df['text'].apply(lambda text: sia.polarity_scores(text)['compound'] if text is not None else 0)\n",
        "\n",
        "# Ajusta las puntuaciones de sentimiento para que se ajusten a la escala de -2 a 2\n",
        "df['sentiment_score_scaled'] = df['sentiment_score'].apply(lambda score: np.interp(score, [-1, 1], [-2, 2]))\n",
        "\n",
        "# Asigna cada rango a una etiqueta de sentimiento\n",
        "bins = [-np.inf, -1.2, -0.2, 0.2, 1.2, np.inf]\n",
        "labels = ['pesimo', 'malo', 'neutral', 'bueno', 'excelente']\n",
        "df['sentiment_label'] = pd.cut(df['sentiment_score_scaled'], bins=bins, labels=labels)\n",
        "\n",
        "# Ahora, la columna 'sentiment_label' contiene la etiqueta de sentimiento para cada texto\n"
      ],
      "metadata": {
        "id": "Z9ApZLlzAfAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtiene una serie con los recuentos de cada categoría de sentimiento\n",
        "recuentos_sentimientos = df_restaurantes_merged['sentiment_label'].value_counts()\n",
        "\n",
        "# Imprime los recuentos de cada categoría de sentimiento\n",
        "print(recuentos_sentimientos)\n"
      ],
      "metadata": {
        "id": "a_G29PEi5c0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
